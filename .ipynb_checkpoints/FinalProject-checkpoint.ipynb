{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project links, files, and basic information\n",
    "\n",
    "### Websites with datasets:\n",
    "- San Diego Vehicle Stops:  https://data.sandiego.gov/datasets/police-vehicle-stops/\n",
    "- Dan Diego Population Data:  http://www.city-data.com/city/San-Diego-California.html\n",
    "\n",
    "### Websites of needed information:\n",
    "- San Diego police service areas https://www.sandiego.gov/police/services/divisions (vehcle stop data only records the first two digits)\n",
    "- San Diego zip code map: http://www.city-data.com/zipmaps/San-Diego-California.html\n",
    "\n",
    "### Names of datasets\n",
    "#### *Vehicle Stops*\n",
    "- 'vehicle_stops_2017.csv'\n",
    "- 'vehicle_stops_2016.csv'\n",
    "- 'vehicle_stops_2015.csv'\n",
    "- 'vehicle_stops_2014.csv'\n",
    "\n",
    "#### *Vehicle Stops Details*\n",
    "- 'vehicle_stops_search_details_2017.csv'\n",
    "- 'vehicle_stops_search_details_2016.csv'\n",
    "- 'vehicle_stops_search_details_2015.csv'\n",
    "- 'vehicle_stops_search_details_2014.csv'\n",
    "\n",
    "#### *Files needed to read Vehicle Stops information*\n",
    "- Race Codes: 'vehicle_stops_race_codes.csv'    \n",
    "- Title explanations for Vehicle Stops data: 'vehicle_stops_dictionary.csv'\n",
    "- Title explanations for Vehicle Stops Details data: 'vehicle_stops_search_details_dictionary.csv'\n",
    "- Possible actions taken when stopped for Vehicle Stops Details data: 'vehicle_stops_search_details_description_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Web scrapping\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} beautifulsoup4\n",
    "\n",
    "# Data analysis\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "\n",
    "import requests\n",
    "!pip install PyPDF2\n",
    "import PyPDF2 as pdf\n",
    "import os\n",
    "import locale\n",
    "#import urllib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulls population chart from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get population data from a specific zip code\n",
    "# Params: code: zip code to be extracted\n",
    "#         percent: percentage of the population of the zip code you are looking into\n",
    "def get_zip_info(code, percent):\n",
    "    locale.setlocale(locale.LC_ALL, '')\n",
    "    currDir = 'zip_pop_data\\\\'\n",
    "    try:\n",
    "        file = currDir + code + '.pdf'\n",
    "        fpdf = pdf.PdfFileReader(file)\n",
    "        page = fpdf.getPage(0).extractText()\n",
    "\n",
    "        # Gets the beginning and end of the data we want\n",
    "        index = page.find('Population\\nPercent\\nTotal Population')\n",
    "        indexEnd = page.find('Source: SANDAG, Current Estimates (2010)\\nPopulation by Race')\n",
    "        text = page[index+19:indexEnd-1]\n",
    "        text = list(text)\n",
    "        for index, item in enumerate(text):\n",
    "            if item == \"\\n\":\n",
    "                text[index] = '/'\n",
    "\n",
    "        text = ''.join(text)\n",
    "        text = text.split('/')\n",
    "        groups = list()\n",
    "        percentages = list()\n",
    "        populations = list()\n",
    "        cols = ['Group', 'Population', 'Percent']\n",
    "\n",
    "        for item in text:\n",
    "            if '%' in item:\n",
    "                percentages.append(item)\n",
    "            elif item[0].isnumeric():\n",
    "                populations.append(locale.atoi(item) * np.float(percent))\n",
    "            else:\n",
    "                groups.append(item) \n",
    "\n",
    "        p_df = pd.DataFrame(columns = cols)\n",
    "        p_df['Group'] = groups\n",
    "        p_df['Population'] = populations\n",
    "        p_df['Percent'] = percentages\n",
    "        p_df.set_index('Group', inplace=True)\n",
    "        p_df = p_df.reindex([\"Total Population\", \"White\", \"Hispanic\", \"Asian\", \"Black\", \"Two or More\", \"American Indian\",\n",
    "                    \"Pacific Islander\", \"Other\"])\n",
    "        p_df.fillna(0.0, inplace=True)\n",
    "        \n",
    "        # Makes sure that each value in percent column has a % sign on it - fixes errors caused by null\n",
    "        \n",
    "        for index, row in p_df.iterrows():\n",
    "            if '%' not in str(row['Percent']):\n",
    "                p_df.loc[index, 'Percent'] = str(row['Percent']) + '%'\n",
    "        return p_df\n",
    "    except PermissionError:\n",
    "        print('error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Cleaning</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Cleaning stops dataframe - Fuctions\n",
    "### Clean unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wanted column titles for stops dataframe\n",
    "stops_col_titles = ['stop_id','stop_cause','service_area','subject_race','subject_sex','subject_age',\n",
    "                    'arrested','searched','contraband_found','property_seized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_cols(stops):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops),stops_col_titles)\n",
    "    stops.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If nans exist of these columns the entry will be dropped\n",
    "clean_nans_cols = ['stop_cause', 'stop_id', 'subject_race', 'subject_sex', 'subject_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get rid of nans vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_nans(stops):\n",
    "    \n",
    "    # Here we assume a Nan means a No in these columns (Since the majority of columns had 'Nan' instead of 'N')\n",
    "    stops['arrested'] = stops['arrested'].replace({np.nan:'N'})\n",
    "    stops['searched'] = stops['searched'].replace({np.nan:'N'})\n",
    "    stops['contraband_found'] = stops['contraband_found'].replace({np.nan:'N'})\n",
    "    stops['property_seized'] = stops['property_seized'].replace({np.nan:'N'})\n",
    "    \n",
    "    stops.dropna(how = 'any', subset = clean_nans_cols, inplace = True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Cleaning stops details dataframe - Functions\n",
    "### Clean unwanted columns of stop details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wanted column titles for stops information dataframe\n",
    "stops_info_col_titles = ['stop_id','search_details_type','search_details_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop informationdataset - Alberto\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_cols(stops_info):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops_info),stops_info_col_titles)\n",
    "    stops_info.drop(drop_list, axis=1, inplace=True) \n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Take out meaningless entry\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_meaningless(stops_info):\n",
    "    \n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTakenOther') \n",
    "                                      & stops_info['search_details_description'].isnull())]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTaken') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'SearchBasis') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standarize action type entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_type(action_type):\n",
    "    action_type = str(action_type)\n",
    "    action_type = action_type.lower()\n",
    "    \n",
    "    if 'action' in action_type:\n",
    "        action_type = 'action'\n",
    "    \n",
    "    elif 'search' in action_type:\n",
    "        action_type = 'search'\n",
    "        \n",
    "    return action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standarize action details entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_desc(action):\n",
    "    \n",
    "    # Otherwise move onto parsinf\n",
    "    action = str(action)\n",
    "    action = action.lower()\n",
    "\n",
    "    if 'arrest' in action:\n",
    "        action = ['arrest']\n",
    "        \n",
    "    elif '310' in action:\n",
    "        action = ['310']\n",
    "        \n",
    "    elif 'imp' in action:\n",
    "        action = ['impound']\n",
    "\n",
    "    elif 'tow' in action:\n",
    "        action = ['tow']\n",
    "        \n",
    "    elif 'mistake' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'released' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'leave' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'free' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no vio' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no dui' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'nothing' in action:\n",
    "        action = ['released']\n",
    "         \n",
    "    elif 'notice' in action:\n",
    "        action = ['suspension notice']\n",
    "        \n",
    "    elif 'plate' in action:\n",
    "        action = ['check plate']\n",
    "        \n",
    "    elif 'passenger' in action:\n",
    "        action = ['passenger']\n",
    "        \n",
    "    elif 'license' in action:\n",
    "        action = ['license']\n",
    "        \n",
    "    elif 'dui' in action:\n",
    "        action = ['dui eval']\n",
    "        \n",
    "    elif 'det' in action:\n",
    "        action = ['detention']\n",
    "        \n",
    "    elif 'contact' in action:\n",
    "        action = ['contact']\n",
    "        \n",
    "    elif 'suspen' in action:\n",
    "        action = ['suspension']\n",
    "    \n",
    "    elif 'susp' in action:\n",
    "        action = ['suspect']\n",
    "        \n",
    "    elif 'cit' in action:\n",
    "        action = ['citation']\n",
    "        \n",
    "    elif 'dmv' in action:\n",
    "        action = ['DMV issue']\n",
    "        \n",
    "    elif 'nan' in action:\n",
    "        action = 'Other'\n",
    "        \n",
    "    else:\n",
    "        action = 'Other'\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean nans and reduce descriptions\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_nans(stops_info):\n",
    "    \n",
    "    # Clean meaningless columns\n",
    "    stops_info = clean_stops_info_meaningless(stops_info)\n",
    "    \n",
    "    # Clean type column\n",
    "    type_title = 'search_details_type'\n",
    "    stops_info[type_title] = stops_info[type_title].apply(standardize_action_type)\n",
    "    \n",
    "    # Clean details column\n",
    "    desc_title = 'search_details_description'\n",
    "    stops_info[desc_title] = stops_info[desc_title].apply(standardize_action_desc)\n",
    "    \n",
    "    # Remove 'Other' and nan entries as they do not give us any extra information\n",
    "    stops_info = stops_info[~(stops_info['search_details_description'] == \"Other\")]\n",
    "    stops_info.dropna(how = 'any', subset = stops_info_col_titles, inplace = True)\n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Final cleaning functions - Combining it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops - stops dataframe to be cleaned\n",
    "def clean_stops(stops):\n",
    "    stops = clean_stops_cols(stops)\n",
    "    stops = clean_stops_nans(stops)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops_info - stops information dataframe to be cleaned\n",
    "def clean_stops_info(stops_info):\n",
    "    stops_info = clean_stops_info_cols(stops_info)\n",
    "    stops_info = clean_stops_info_nans(stops_info)\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Merging stops and details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function: Merges duplicates within the information dataset\n",
    "# Params: info - dataframe with stops information\n",
    "def merge_duplicates(info):\n",
    "    \n",
    "    deleted = 0\n",
    "    last_index = len(info) -1\n",
    "\n",
    "    for index, row in info.iterrows():\n",
    "    \n",
    "        if deleted > 0:\n",
    "            deleted -= 1\n",
    "        \n",
    "        elif index < last_index:\n",
    "        \n",
    "            s_id = row['stop_id']\n",
    "        \n",
    "            next_index = index+1\n",
    "            next_id = info['stop_id'][next_index]\n",
    "    \n",
    "            while (s_id == next_id) & (next_index <= last_index):\n",
    "            \n",
    "                # Grab entry of duplicate\n",
    "                entry = info.loc[next_index, 'search_details_description']\n",
    "            \n",
    "                # Append duplicate entry to original\n",
    "                info.loc[index, 'search_details_description'].append(entry[0])\n",
    "            \n",
    "                # Drop duplicate row\n",
    "                info.drop(next_index, inplace=True)\n",
    "            \n",
    "                # Increase index of next row\n",
    "                next_index += 1\n",
    "            \n",
    "                # Check for out of bounds\n",
    "                if next_index  < last_index:\n",
    "                    next_id = info['stop_id'][next_index]\n",
    "                \n",
    "                deleted += 1\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Merge the stops and details dataframes\n",
    "# Params: stops - dataframe with stops information\n",
    "#          info - dataframe with stop details\n",
    "def merge_dataframes(stops, info):\n",
    "    \n",
    "    \n",
    "    # Drop type information\n",
    "    info.drop('search_details_type', axis=1, inplace=True)\n",
    "    \n",
    "    # Reset indeces\n",
    "    info = info.reset_index()\n",
    "    info.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    # Merge duplicates of information dataset\n",
    "    info = merge_duplicates(info)\n",
    "    \n",
    "    df_merged = stops.merge(info, on = ['stop_id'], how = 'left')\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Seting up for Data Analysis</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Mapping functions and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of individuals' races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that maps all of the police race data into categories given in census\n",
    "# Param: race - character correcponding to a race to be assigned\n",
    "# Return: (Based on census) A = asian, B = black, H = hispanic, I = indian, O = other\n",
    "def assign_race(race):\n",
    "    \n",
    "    if race in ['A','C','D','F','J','K','L','V']:\n",
    "        return 'A'\n",
    "    \n",
    "    elif race == 'B':\n",
    "        return 'B'\n",
    "    \n",
    "    elif race == 'H':\n",
    "        return 'H'\n",
    "    \n",
    "    elif race == 'W':\n",
    "        return 'W'\n",
    "    \n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of police area to zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that maps zip codes to police areas\n",
    "def get_area_zips(area):\n",
    "    code_dict = {'110':list(['92122', '1', '92117', '1', '92111', '0.8', '92110', '0.4' ]),\n",
    "                '120': list(['92109', '1', '92037', '1']),\n",
    "                '130': list(['0']),\n",
    "                '230': list(['92129', '1', '92128', '1', '92127', '0.3', '92025', '0.3']),\n",
    "                '240': list(['92145', '1', '92126', '1', '92131', '1']),\n",
    "                '310': list(['92123', '1', '92124', '1', '92108', '1', '92111', '0.2']),\n",
    "                '320': list(['92120', '1', '92119', '1']),\n",
    "                '430': list(['92139', '1', '92114', '1']),\n",
    "                '440': list(['92136', '1', '92102', '0.4', '92113', '0.5']),\n",
    "                '510': list(['92113', '0.5', '92102', '0.6']),\n",
    "                '520': list(['92101', '1']),\n",
    "                '530': list(['0']),\n",
    "                '610': list(['92107', '1', '92106', '1', '92140', '1', '92110', '0.6']),\n",
    "                '620': list(['92103', '1']),\n",
    "                '630': list(['0']),\n",
    "                '710': list(['92173', '1', '92154', '0.4']),\n",
    "                '720': list(['92154', '0.6']),\n",
    "                '810': list(['0']),\n",
    "                '820': list(['92115', '1', '92116', '0.6']),\n",
    "                '830': list(['92105', '0.8']),\n",
    "                '840': list(['0']),\n",
    "                '930': list(['92121', '1', '92130', '1', '92014', '1', '92091', '1', '92127', '0.7']),\n",
    "                'Unknown': list(['0'])\n",
    "                }\n",
    "    return code_dict[area]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of possible police actions taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of actions that can appear in merged stops dataframes \n",
    "actions = list(['arrest', '310', 'impound', 'tow', 'released', 'suspension notice', 'check plate', 'passenger',\n",
    "                'license', 'dui eval', 'detention', 'contact', 'suspension', 'suspect', 'citation', 'DMV issue', \n",
    "                'other', 'NaN', 'total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Races that make up the majority of the san diego area\n",
    "races = list(['W', 'H', 'A', 'B', 'O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Building data-structures for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to hold information about actions and population for every police area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - Sets up police area dictionary for a given year\n",
    "# Return: Dictionary where each police code has two empty dataframes\n",
    "#         The dataframes will correspond to police actions per race and population percentages for each area\n",
    "def get_year_areas():\n",
    "    df_areas = {\n",
    "        '110': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '120': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '130': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '230': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '240': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '310': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '320': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '430': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '440': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '510': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '520': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '530': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '610': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '620': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '630': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '710': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '720': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '810': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '820': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '830': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '840': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '930': [pd.DataFrame(), pd.DataFrame()],\n",
    "        'Unknown': [pd.DataFrame(), pd.DataFrame()]\n",
    "    }\n",
    "    \n",
    "    return df_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to fill in information into above dataframe per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Fills in the first dataframe at the given year with the total\n",
    "#           number of police actions for each race in each police area\n",
    "# Params: year - year whose dataframe (containing sum of actions per race) will be filled\n",
    "#         year_df - dataframe to be filled\n",
    "def get_code_race_data(year, year_df):\n",
    "    \n",
    "    # Columns corresponding to races\n",
    "    cols = ['W', 'B', 'A', 'H', 'O']\n",
    "    \n",
    "    # Initialize dataframe with action and race columns for each police area in the year\n",
    "    for current_area in year_df:\n",
    "        year_df[current_area][0] = pd.DataFrame(columns = cols)\n",
    "        year_df[current_area][0]['Action'] = actions\n",
    "        year_df[current_area][0].fillna(0, inplace=True)\n",
    "        year_df[current_area][0].set_index('Action', inplace=True)\n",
    "\n",
    "    # Counts different actions for every area\n",
    "    for index, row in years[year].iterrows():\n",
    "        # Catch any exceptions - for our error checking\n",
    "        try:\n",
    "            race = assign_race(row['subject_race'])\n",
    "        except KeyError:\n",
    "            print(row['subject_race'])\n",
    "            continue\n",
    "            \n",
    "        desc = row['search_details_description']\n",
    "        area = row['service_area']\n",
    "        if desc is not np.nan:\n",
    "            for item in desc:\n",
    "                year_df[area][0].loc[item, race] += 1\n",
    "            \n",
    "    # Sums up total total\n",
    "    for item in cols:\n",
    "        for current_area in year_df:\n",
    "            year_df[current_area][0].loc['total', item] = year_df[current_area][0][item].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function: Fills in the second dataframe at the given year with\n",
    "#           the total population counts for eachpolice area\n",
    "# Params: year - year whose dataframe of total population counts will be filled\n",
    "#         year_df - dataframe to be filled\n",
    "\n",
    "def fill_area_pop_data(year, year_df):\n",
    "    for area in year_df:\n",
    "        codes = get_area_zips(area)\n",
    "        if len(codes) is not 1:\n",
    "            df_total = get_zip_info(codes[0], codes[1])\n",
    "            \n",
    "            # Since each police area covers multiple zip codes, we must loop through all codes in each area\n",
    "            for index in range(2,len(codes)-1, 2):\n",
    "                df_temp = get_zip_info(codes[index], codes[index+1])\n",
    "                df_total = df_total.add(df_temp, fill_value=0)\n",
    "\n",
    "            for index, row in df_total.iterrows():\n",
    "                # Calculates the new percentages of the added zip codes\n",
    "                pop = np.float((np.float(row['Population']) / np.float(df_total['Population'][0] * 100)))\n",
    "                df_total.loc[index, 'Percent'] = np.float(pop) * 1000\n",
    "                # Turning Populations into ints\n",
    "                df_total.loc[index, 'Population'] = int(df_total.loc[index, 'Population'])\n",
    "\n",
    "            year_df[area][1] = df_total\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Calculates total number actions from a given year's dataframe\n",
    "# Params: year - year whose dataframe we are looking into\n",
    "#         currArea - police area in dataframe we are counting\n",
    "#         df - dataframe of corresponding year we are looking into\n",
    "def get_total(year, currArea, df):\n",
    "    total = 0\n",
    "    for item in df[currArea][0].columns:\n",
    "        total += df[currArea][0].loc['total', item]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Reading</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Read, clean, and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and clean stops datasets and clean\n",
    "df_stops_17 = clean_stops(pd.read_csv('vehicle_stops_2017.csv'))\n",
    "df_stops_16 = clean_stops(pd.read_csv('vehicle_stops_2016.csv'))\n",
    "df_stops_15 = clean_stops(pd.read_csv('vehicle_stops_2015.csv'))\n",
    "df_stops_14 = clean_stops(pd.read_csv('vehicle_stops_2014.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and clean stop details datasets\n",
    "df_stops_info_17 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2017.csv'))\n",
    "df_stops_info_16 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2016.csv'))\n",
    "df_stops_info_15 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2015.csv'))\n",
    "df_stops_info_14 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2014.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge above datasets \n",
    "df_merged_17 = merge_dataframes(df_stops_17, df_stops_info_17)\n",
    "df_merged_16 = merge_dataframes(df_stops_16, df_stops_info_16)\n",
    "df_merged_15 = merge_dataframes(df_stops_16, df_stops_info_15)\n",
    "df_merged_14 = merge_dataframes(df_stops_14, df_stops_info_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store merged datasets for easier access\n",
    "years = {\n",
    "    '2017': df_merged_17,\n",
    "    '2016': df_merged_16,\n",
    "    '2015': df_merged_15,\n",
    "    '2014': df_merged_14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_cause</th>\n",
       "      <th>service_area</th>\n",
       "      <th>subject_race</th>\n",
       "      <th>subject_sex</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>arrested</th>\n",
       "      <th>searched</th>\n",
       "      <th>contraband_found</th>\n",
       "      <th>property_seized</th>\n",
       "      <th>search_details_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1444799</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>120</td>\n",
       "      <td>I</td>\n",
       "      <td>M</td>\n",
       "      <td>37</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1444821</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>520</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>22</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1447102</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>520</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1444801</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>720</td>\n",
       "      <td>H</td>\n",
       "      <td>F</td>\n",
       "      <td>61</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1444802</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>120</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stop_id           stop_cause service_area subject_race subject_sex  \\\n",
       "0  1444799     Moving Violation          120            I           M   \n",
       "1  1444821  Equipment Violation          520            W           M   \n",
       "2  1447102     Moving Violation          520            W           M   \n",
       "3  1444801  Equipment Violation          720            H           F   \n",
       "4  1444802  Equipment Violation          120            H           M   \n",
       "\n",
       "  subject_age arrested searched contraband_found property_seized  \\\n",
       "0          37        N        N                N               N   \n",
       "1          22        N        N                N               N   \n",
       "2          29        N        N                N               N   \n",
       "3          61        N        N                N               N   \n",
       "4          24        N        N                N               N   \n",
       "\n",
       "  search_details_description  \n",
       "0                 [citation]  \n",
       "1                        NaN  \n",
       "2                 [citation]  \n",
       "3                        NaN  \n",
       "4                        NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at 2017 dataset\n",
    "years['2017'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Fill in datasets with data analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' Bulletin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-113a9c3a9123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_2014\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_year_areas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_code_race_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mget_code_race_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2016'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_2016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_code_race_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2015'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_2015\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-416162af00bf>\u001b[0m in \u001b[0;36mget_code_race_data\u001b[0;34m(year, year_df)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0myear_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marea\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrace\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Sums up total total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ' Bulletin'"
     ]
    }
   ],
   "source": [
    "# Loads in all the data at once. Takes roughly 6 mins to run, but will make the rest of the program much faster \n",
    "df_2017 = get_year_areas()\n",
    "df_2016 = get_year_areas()\n",
    "df_2015 = get_year_areas()\n",
    "df_2014 = get_year_areas()\n",
    "\n",
    "get_code_race_data('2017', df_2017)\n",
    "get_code_race_data('2016', df_2016)\n",
    "get_code_race_data('2015', df_2015)\n",
    "get_code_race_data('2014', df_2014)\n",
    "\n",
    "fill_area_pop_data('2017', df_2017)\n",
    "fill_area_pop_data('2016', df_2016)\n",
    "fill_area_pop_data('2015', df_2015)\n",
    "fill_area_pop_data('2014', df_2014)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_2017['930'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('vehicle_stops_search_details_2017.csv')\n",
    "df1 = pd.read_csv('vehicle_stops_2017.csv')\n",
    "df3 = df2.merge(df1,on='stop_id')\n",
    "df4 = df3[df3['service_area']=='930']\n",
    "df4 = df4[df4['subject_race']=='W']\n",
    "len(df4[df4['search_details_description']=='Citation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_years = {\n",
    "    '2017':df_2017,\n",
    "    '2016':df_2016,\n",
    "    '2015':df_2015,\n",
    "    '2014':df_2014\n",
    "}\n",
    "\n",
    "print(df_years['2017']['930'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Compares the number of actions taken against the given race vs. the population of that race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def race_compare(area, race, year):\n",
    "    df_pop = df_years[year][area][1]\n",
    "    df_actions = df_years[year][area][0]\n",
    "    total = get_total(year, area, df_years[year])\n",
    "    other = np.float(df_pop.loc['Other', 'Population'] + df_pop.loc['Pacific Islander', 'Population'] + \n",
    "                    df_pop.loc['American Indian', 'Population'] + df_pop.loc['Two or More', 'Population'] /\n",
    "                    df_pop.loc['Total Population', 'Population'])\n",
    "    \n",
    "    switch = {\n",
    "        'W': np.float(df_actions.loc['total', race] / total / np.float(df_pop.loc['White', 'Percent'])),#df_pop.loc['White', 'Population'] / df_actions.loc['total', race],\n",
    "        'B': np.float(df_actions.loc['total', race] / total / np.float(df_pop.loc['Black', 'Percent'])),\n",
    "       # 'I': np.float((df_actions.loc['total', race] / total)) / np.float(df_pop.loc['American Indian', 'Percent']),\n",
    "        'A': np.float(df_actions.loc['total', race] / total) / np.float(df_pop.loc['Asian', 'Percent']),\n",
    "        'H': np.float(df_actions.loc['total', race] / total) / np.float(df_pop.loc['Hispanic', 'Percent']),\n",
    "        'O': np.float(df_actions.loc['total', race] / total) / np.float(df_pop.loc['Other', 'Percent']),\n",
    "        'I': 0\n",
    "    }\n",
    "    \n",
    "    return switch[race] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.float(df_years['2017']['110'][1].loc['Asian', 'Percent']) / (df_years['2017']['110'][0].loc['total', 'A'] / get_total('2017', '110', df_years['2017'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.float(df_years['2017']['110'][1].loc['Black', 'Percent']))\n",
    "print(df_years['2017']['110'][0].loc['total', 'B'] / get_total('2017', '110', df_years['2017'])) \n",
    "print('\\n')\n",
    "\n",
    "print(df_years['2017']['110'][0].loc['total', 'B'])\n",
    "print(get_total('2017', '110', df_years['2017']))\n",
    "print(7/17)\n",
    "print(df_years['2017']['110'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = list()\n",
    "for race in races:\n",
    "    ratios.append(race_compare('110', race, '2017'))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(races, ratios, width=.75)\n",
    "\n",
    "#print(df_years['2016']['930'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_years = ['2014', '2015', '2016', '2017']\n",
    "area_lookup = '110'\n",
    "whites = list()\n",
    "asians = list()\n",
    "hispanics = list()\n",
    "blacks = list()\n",
    "\n",
    "for year in graph_years:\n",
    "    whites.append(race_compare(area_lookup, 'W', year))\n",
    "    asians.append(race_compare(area_lookup, 'A', year))\n",
    "    hispanics.append(race_compare(area_lookup, 'H', year))\n",
    "    blacks.append(race_compare(area_lookup, 'B', year))\n",
    "    \n",
    "    \n",
    "plt.plot(graph_years, whites, 'ro')\n",
    "plt.plot(graph_years, asians, 'yo')\n",
    "plt.plot(graph_years, hispanics, 'go')\n",
    "plt.plot(graph_years, blacks, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
