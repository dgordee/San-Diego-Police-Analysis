{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project links, files, and basic information\n",
    "\n",
    "### Websites with datasets:\n",
    "- San Diego Vehicle Stops:  https://data.sandiego.gov/datasets/police-vehicle-stops/\n",
    "- Dan Diego Population Data:  http://www.city-data.com/city/San-Diego-California.html\n",
    "\n",
    "### Websites of needed information:\n",
    "- San Diego police service areas https://www.sandiego.gov/police/services/divisions (vehcle stop data only records the first two digits)\n",
    "- San Diego zip code map: http://www.city-data.com/zipmaps/San-Diego-California.html\n",
    "\n",
    "### Names of datasets\n",
    "#### *Vehicle Stops*\n",
    "- 'vehicle_stops_2017.csv'\n",
    "- 'vehicle_stops_2016.csv'\n",
    "- 'vehicle_stops_2015.csv'\n",
    "- 'vehicle_stops_2014.csv'\n",
    "\n",
    "#### *Vehicle Stops Details*\n",
    "- 'vehicle_stops_search_details_2017.csv'\n",
    "- 'vehicle_stops_search_details_2016.csv'\n",
    "- 'vehicle_stops_search_details_2015.csv'\n",
    "- 'vehicle_stops_search_details_2014.csv'\n",
    "\n",
    "#### *Files needed to read Vehicle Stops information*\n",
    "- Race Codes: 'vehicle_stops_race_codes.csv'    \n",
    "- Title explanations for Vehicle Stops data: 'vehicle_stops_dictionary.csv'\n",
    "- Title explanations for Vehicle Stops Details data: 'vehicle_stops_search_details_dictionary.csv'\n",
    "- Possible actions taken when stopped for Vehicle Stops Details data: 'vehicle_stops_search_details_description_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data analysis\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "\n",
    "# Interactable \n",
    "from ipywidgets import interactive\n",
    "\n",
    "# PDF Reading\n",
    "!pip install PyPDF2\n",
    "import PyPDF2 as pdf\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Cleaning</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Cleaning stops dataframe - Fuctions\n",
    "### Clean unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wanted column titles for stops dataframe\n",
    "stops_col_titles = ['stop_id','stop_cause','service_area','subject_race','subject_sex','subject_age',\n",
    "                    'arrested','searched','contraband_found','property_seized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_cols(stops):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops),stops_col_titles)\n",
    "    stops.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If nans exist of these columns the entry will be dropped\n",
    "clean_nans_cols = ['stop_cause', 'stop_id', 'subject_race', 'subject_sex', 'subject_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get rid of nans vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_nans(stops):\n",
    "    \n",
    "    # Here we assume a Nan means a No in these columns (Since the majority of columns had 'Nan' instead of 'N')\n",
    "    stops['arrested'] = stops['arrested'].replace({np.nan:'N'})\n",
    "    stops['searched'] = stops['searched'].replace({np.nan:'N'})\n",
    "    stops['contraband_found'] = stops['contraband_found'].replace({np.nan:'N'})\n",
    "    stops['property_seized'] = stops['property_seized'].replace({np.nan:'N'})\n",
    "    \n",
    "    stops.dropna(how = 'any', subset = clean_nans_cols, inplace = True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Cleaning stops details dataframe - Functions\n",
    "### Clean unwanted columns of stop details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wanted column titles for stops information dataframe\n",
    "stops_info_col_titles = ['stop_id','search_details_type','search_details_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop informationdataset - Alberto\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_cols(stops_info):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops_info),stops_info_col_titles)\n",
    "    stops_info.drop(drop_list, axis=1, inplace=True) \n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Take out meaningless entry\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_meaningless(stops_info):\n",
    "    \n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTakenOther') \n",
    "                                      & stops_info['search_details_description'].isnull())]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTaken') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'SearchBasis') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standarize action type entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_type(action_type):\n",
    "    action_type = str(action_type)\n",
    "    action_type = action_type.lower()\n",
    "    \n",
    "    if 'action' in action_type:\n",
    "        action_type = 'action'\n",
    "    \n",
    "    elif 'search' in action_type:\n",
    "        action_type = 'search'\n",
    "        \n",
    "    return action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standarize action details entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_desc(action):\n",
    "    \n",
    "    # Otherwise move onto parsinf\n",
    "    action = str(action)\n",
    "    action = action.lower()\n",
    "\n",
    "    if 'arrest' in action:\n",
    "        action = ['arrest']\n",
    "        \n",
    "    elif '310' in action:\n",
    "        action = ['310']\n",
    "        \n",
    "    elif 'imp' in action:\n",
    "        action = ['impound']\n",
    "\n",
    "    elif 'tow' in action:\n",
    "        action = ['tow']\n",
    "        \n",
    "    elif 'mistake' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'released' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'leave' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'free' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no vio' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no dui' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'nothing' in action:\n",
    "        action = ['released']\n",
    "         \n",
    "    elif 'notice' in action:\n",
    "        action = ['suspension notice']\n",
    "        \n",
    "    elif 'plate' in action:\n",
    "        action = ['check plate']\n",
    "        \n",
    "    elif 'passenger' in action:\n",
    "        action = ['passenger']\n",
    "        \n",
    "    elif 'license' in action:\n",
    "        action = ['license']\n",
    "        \n",
    "    elif 'dui' in action:\n",
    "        action = ['dui eval']\n",
    "        \n",
    "    elif 'det' in action:\n",
    "        action = ['detention']\n",
    "        \n",
    "    elif 'contact' in action:\n",
    "        action = ['contact']\n",
    "        \n",
    "    elif 'suspen' in action:\n",
    "        action = ['suspension']\n",
    "    \n",
    "    elif 'susp' in action:\n",
    "        action = ['suspect']\n",
    "        \n",
    "    elif 'cit' in action:\n",
    "        action = ['citation']\n",
    "        \n",
    "    elif 'dmv' in action:\n",
    "        action = ['DMV issue']\n",
    "        \n",
    "    elif 'nan' in action:\n",
    "        action = 'Other'\n",
    "        \n",
    "    else:\n",
    "        action = 'Other'\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean nans and reduce descriptions\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_nans(stops_info):\n",
    "    \n",
    "    # Clean meaningless columns\n",
    "    stops_info = clean_stops_info_meaningless(stops_info)\n",
    "    \n",
    "    # Clean type column\n",
    "    type_title = 'search_details_type'\n",
    "    stops_info[type_title] = stops_info[type_title].apply(standardize_action_type)\n",
    "    \n",
    "    # Clean details column\n",
    "    desc_title = 'search_details_description'\n",
    "    stops_info[desc_title] = stops_info[desc_title].apply(standardize_action_desc)\n",
    "    \n",
    "    # Remove 'Other' and nan entries as they do not give us any extra information\n",
    "    stops_info = stops_info[~(stops_info['search_details_description'] == \"Other\")]\n",
    "    stops_info.dropna(how = 'any', subset = stops_info_col_titles, inplace = True)\n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Final cleaning functions - Combining it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops - stops dataframe to be cleaned\n",
    "def clean_stops(stops):\n",
    "    stops = clean_stops_cols(stops)\n",
    "    stops = clean_stops_nans(stops)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops_info - stops information dataframe to be cleaned\n",
    "def clean_stops_info(stops_info):\n",
    "    stops_info = clean_stops_info_cols(stops_info)\n",
    "    stops_info = clean_stops_info_nans(stops_info)\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Merging stops and details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function: Merges duplicates within the information dataset\n",
    "# Params: info - dataframe with stops information\n",
    "def merge_duplicates(info):\n",
    "    \n",
    "    deleted = 0\n",
    "    last_index = len(info) -1\n",
    "\n",
    "    for index, row in info.iterrows():\n",
    "    \n",
    "        if deleted > 0:\n",
    "            deleted -= 1\n",
    "        \n",
    "        elif index < last_index:\n",
    "        \n",
    "            s_id = row['stop_id']\n",
    "        \n",
    "            next_index = index+1\n",
    "            next_id = info['stop_id'][next_index]\n",
    "    \n",
    "            while (s_id == next_id) & (next_index <= last_index):\n",
    "            \n",
    "                # Grab entry of duplicate\n",
    "                entry = info.loc[next_index, 'search_details_description']\n",
    "            \n",
    "                # Append duplicate entry to original\n",
    "                info.loc[index, 'search_details_description'].append(entry[0])\n",
    "            \n",
    "                # Drop duplicate row\n",
    "                info.drop(next_index, inplace=True)\n",
    "            \n",
    "                # Increase index of next row\n",
    "                next_index += 1\n",
    "            \n",
    "                # Check for out of bounds\n",
    "                if next_index  < last_index:\n",
    "                    next_id = info['stop_id'][next_index]\n",
    "                \n",
    "                deleted += 1\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Merge the stops and details dataframes\n",
    "# Params: stops - dataframe with stops information\n",
    "#          info - dataframe with stop details\n",
    "def merge_dataframes(stops, info):\n",
    "    \n",
    "    \n",
    "    # Drop type information\n",
    "    info.drop('search_details_type', axis=1, inplace=True)\n",
    "    \n",
    "    # Reset indeces\n",
    "    info = info.reset_index()\n",
    "    info.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    # Merge duplicates of information dataset\n",
    "    info = merge_duplicates(info)\n",
    "    \n",
    "    df_merged = stops.merge(info, on = ['stop_id'], how = 'left')\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Seting up for Data Analysis</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Mapping functions and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of individuals' races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that maps all of the police race data into categories given in census\n",
    "# Param: race - character correcponding to a race to be assigned\n",
    "# Return: (Based on census) A = asian, B = black, H = hispanic, I = indian, O = other\n",
    "def assign_race(race):\n",
    "    \n",
    "    if race in ['A','C','D','F','J','K','L','V','I']:\n",
    "        return 'A'\n",
    "    \n",
    "    elif race == 'B':\n",
    "        return 'B'\n",
    "    \n",
    "    elif race == 'H':\n",
    "        return 'H'\n",
    "    \n",
    "    elif race == 'W':\n",
    "        return 'W'\n",
    "    \n",
    "    elif race in ['E','G','M','N','O','P','Q','R','S','T','U','Y','Z']:\n",
    "        return 'O'\n",
    "    \n",
    "    else:\n",
    "        return 'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of police area to zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that maps zip codes to police areas\n",
    "def get_area_zips(area):\n",
    "    code_dict = {'110':list(['92122', '1', '92117', '1', '92111', '0.8', '92110', '0.4' ]),\n",
    "                '120': list(['92109', '1', '92037', '1']),\n",
    "                '130': list(['0']),\n",
    "                '230': list(['92129', '1', '92128', '1', '92127', '0.3', '92025', '0.3']),\n",
    "                '240': list(['92145', '1', '92126', '1', '92131', '1']),\n",
    "                '310': list(['92123', '1', '92124', '1', '92108', '1', '92111', '0.2']),\n",
    "                '320': list(['92120', '1', '92119', '1']),\n",
    "                '430': list(['92139', '1', '92114', '1']),\n",
    "                '440': list(['92136', '1', '92102', '0.4', '92113', '0.5']),\n",
    "                '510': list(['92113', '0.5', '92102', '0.6']),\n",
    "                '520': list(['92101', '1']),\n",
    "                '530': list(['0']),\n",
    "                '610': list(['92107', '1', '92106', '1', '92140', '1', '92110', '0.6']),\n",
    "                '620': list(['92103', '1']),\n",
    "                '630': list(['0']),\n",
    "                '710': list(['92173', '1', '92154', '0.4']),\n",
    "                '720': list(['92154', '0.6']),\n",
    "                '810': list(['0']),\n",
    "                '820': list(['92115', '1', '92116', '0.6']),\n",
    "                '830': list(['92105', '0.8']),\n",
    "                '840': list(['0']),\n",
    "                '930': list(['92121', '1', '92130', '1', '92014', '1', '92091', '1', '92127', '0.7']),\n",
    "                'Unknown': list(['0'])\n",
    "                }\n",
    "    return code_dict[area]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of possible police actions taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of actions that can appear in merged stops dataframes \n",
    "actions = list(['arrest', '310', 'impound', 'tow', 'released', 'suspension notice', 'check plate', 'passenger',\n",
    "                'license', 'dui eval', 'detention', 'contact', 'suspension', 'suspect', 'citation', 'DMV issue', \n",
    "                'other', 'NaN', 'total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years_list = ['2017', '2016', '2015', '2014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of police codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "areas_codes_list = ['110','120','230','240','310','320','430','440','510','520','610','620','710','720','820','830','930']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Races that make up the majority of the san diego area\n",
    "races = list(['W', 'H', 'A', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> PDF Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get population percentages by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get population data from a specific zip code\n",
    "# Params: code: zip code to be extracted\n",
    "#         percent: percentage of the population of the zip code you are looking into\n",
    "def get_zip_info(code, percent):\n",
    "    locale.setlocale(locale.LC_ALL, '')\n",
    "    currDir = 'zip_pop_data/'\n",
    "    try:\n",
    "        file = currDir + code + '.pdf'\n",
    "        fpdf = pdf.PdfFileReader(file)\n",
    "        page = fpdf.getPage(0).extractText()\n",
    "\n",
    "        # Gets the beginning and end of the data we want\n",
    "        index = page.find('Population\\nPercent\\nTotal Population')\n",
    "        indexEnd = page.find('Source: SANDAG, Current Estimates (2010)\\nPopulation by Race')\n",
    "        text = page[index+19:indexEnd-1]\n",
    "        text = list(text)\n",
    "        for index, item in enumerate(text):\n",
    "            if item == \"\\n\":\n",
    "                text[index] = '/'\n",
    "\n",
    "        text = ''.join(text)\n",
    "        text = text.split('/')\n",
    "        groups = list()\n",
    "        percentages = list()\n",
    "        populations = list()\n",
    "        cols = ['Group', 'Population', 'Percent']\n",
    "\n",
    "        for item in text:\n",
    "            if '%' in item:\n",
    "                percentages.append(item)\n",
    "            elif item[0].isnumeric():\n",
    "                populations.append(locale.atoi(item) * np.float(percent))\n",
    "            else:\n",
    "                groups.append(item) \n",
    "\n",
    "        p_df = pd.DataFrame(columns = cols)\n",
    "        p_df['Group'] = groups\n",
    "        p_df['Population'] = populations\n",
    "        p_df['Percent'] = percentages\n",
    "        p_df.set_index('Group', inplace=True)\n",
    "        p_df = p_df.reindex([\"Total Population\", \"White\", \"Hispanic\", \"Asian\", \"Black\", \"Two or More\", \"American Indian\",\n",
    "                    \"Pacific Islander\", \"Other\"])\n",
    "        p_df.fillna(0.0, inplace=True)\n",
    "        \n",
    "        # Makes sure that each value in percent column has a % sign on it - fixes errors caused by null\n",
    "        \n",
    "        for index, row in p_df.iterrows():\n",
    "            if '%' not in str(row['Percent']):\n",
    "                p_df.loc[index, 'Percent'] = str(row['Percent']) + '%'\n",
    "        return p_df\n",
    "    except PermissionError:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Population Percent\n",
      "Group                               \n",
      "Total Population     43382.0    100%\n",
      "White                23612.0     54%\n",
      "Hispanic              4083.0      9%\n",
      "Asian                13321.0     31%\n",
      "Black                  635.0      1%\n",
      "Two or More           1464.0      3%\n",
      "American Indian         56.0     <1%\n",
      "Pacific Islander        72.0     <1%\n",
      "Other                  139.0     <1%\n"
     ]
    }
   ],
   "source": [
    "print(get_zip_info('92122', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Building data-structures for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to hold information about actions and population for every police area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function - Sets up police area dictionary for a given year\n",
    "# Return: Dictionary where each police code has two empty dataframes\n",
    "#         The dataframes will correspond to police actions per race and population percentages for each area\n",
    "def get_year_areas():\n",
    "    df_areas = {\n",
    "        '110': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '120': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '130': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '230': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '240': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '310': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '320': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '430': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '440': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '510': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '520': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '530': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '610': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '620': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '630': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '710': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '720': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '810': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '820': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '830': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '840': [pd.DataFrame(), pd.DataFrame()],\n",
    "        '930': [pd.DataFrame(), pd.DataFrame()],\n",
    "        'Unknown': [pd.DataFrame(), pd.DataFrame()]\n",
    "    }\n",
    "    \n",
    "    return df_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to fill in information into above dataframe per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Fills in the first dataframe at the given year with the total\n",
    "#           number of police actions for each race in each police area\n",
    "# Params: year - year whose dataframe (containing sum of actions per race) will be filled\n",
    "#         year_df - dataframe to be filled\n",
    "def get_code_race_data(year, year_df):\n",
    "    \n",
    "    # Columns corresponding to races\n",
    "    cols = ['W', 'B', 'A', 'H','O']\n",
    "    \n",
    "    # Initialize dataframe with action and race columns for each police area in the year\n",
    "    for current_area in year_df:\n",
    "        year_df[current_area][0] = pd.DataFrame(columns = cols)\n",
    "        year_df[current_area][0]['Action'] = actions\n",
    "        year_df[current_area][0].fillna(0, inplace=True)\n",
    "        year_df[current_area][0].set_index('Action', inplace=True)\n",
    "\n",
    "    # Counts different actions for every area\n",
    "    for index, row in years[year].iterrows():\n",
    "        \n",
    "        race = assign_race(row['subject_race'])\n",
    "        if race == 'X':\n",
    "            continue\n",
    "            \n",
    "        desc = row['search_details_description']\n",
    "        area = row['service_area']\n",
    "        if desc is not np.nan:\n",
    "            for item in desc:\n",
    "                year_df[area][0].loc[item, race] += 1\n",
    "                \n",
    "    # Sums up total total\n",
    "    for item in cols:\n",
    "        for current_area in year_df:\n",
    "            year_df[current_area][0].loc['total', item] = year_df[current_area][0][item].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function: Fills in the second dataframe at the given year with\n",
    "#           the total population counts for eachpolice area\n",
    "# Params: year - year whose dataframe of total population counts will be filled\n",
    "#         year_df - dataframe to be filled\n",
    "\n",
    "def fill_area_pop_data(year, year_df):\n",
    "    for area in year_df:\n",
    "        codes = get_area_zips(area)\n",
    "        if len(codes) is not 1:\n",
    "            df_total = get_zip_info(codes[0], codes[1])\n",
    "            \n",
    "            # Since each police area covers multiple zip codes, we must loop through all codes in each area\n",
    "            for index in range(2,len(codes)-1, 2):\n",
    "                df_temp = get_zip_info(codes[index], codes[index+1])\n",
    "                df_total = df_total.add(df_temp, fill_value=0)\n",
    "\n",
    "            for index, row in df_total.iterrows():\n",
    "                # Calculates the new percentages of the added zip codes\n",
    "                pop = np.float((np.float(row['Population']) / np.float(df_total['Population'][0] * 100)))\n",
    "                df_total.loc[index, 'Percent'] = np.float(pop) * 10000\n",
    "                # Turning Populations into ints\n",
    "                df_total.loc[index, 'Population'] = int(df_total.loc[index, 'Population'])\n",
    "\n",
    "            year_df[area][1] = df_total\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function: Calculates total number actions from a given year's dataframe\n",
    "# Params: year - year whose dataframe we are looking into\n",
    "#         currArea - police area in dataframe we are counting\n",
    "#         df - dataframe of corresponding year we are looking into\n",
    "def get_total(year, currArea, df):\n",
    "    total = 0\n",
    "    for item in df[currArea][0].columns:\n",
    "        total += df[currArea][0].loc['total', item]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Reading</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Read, clean, and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and clean stops datasets and clean\n",
    "df_stops_17 = clean_stops(pd.read_csv('vehicle_stops_2017.csv'))\n",
    "df_stops_16 = clean_stops(pd.read_csv('vehicle_stops_2016.csv'))\n",
    "df_stops_15 = clean_stops(pd.read_csv('vehicle_stops_2015.csv'))\n",
    "df_stops_14 = clean_stops(pd.read_csv('vehicle_stops_2014.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and clean stop details datasets\n",
    "df_stops_info_17 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2017.csv'))\n",
    "df_stops_info_16 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2016.csv'))\n",
    "df_stops_info_15 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2015.csv'))\n",
    "df_stops_info_14 = clean_stops_info(pd.read_csv('vehicle_stops_search_details_2014.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-182e280213c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merge above datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_merged_17\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stops_17\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stops_info_17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_merged_16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stops_16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stops_info_16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_merged_15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stops_15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stops_info_15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_merged_14\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stops_14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stops_info_14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-898bd80b7c63>\u001b[0m in \u001b[0;36mmerge_dataframes\u001b[0;34m(stops, info)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Merge duplicates of information dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdf_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'stop_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e4e04532dfa9>\u001b[0m in \u001b[0;36mmerge_duplicates\u001b[0;34m(info)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlast_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeleted\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 248\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   2950\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2952\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path)\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2918\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2919\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2921\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_cast_to_datetime\u001b[0;34m(value, dtype, errors)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0mnan\u001b[0m \u001b[0mto\u001b[0m \u001b[0miNaT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \"\"\"\n\u001b[0;32m--> 872\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedeltas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_timedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Merge above datasets \n",
    "df_merged_17 = merge_dataframes(df_stops_17, df_stops_info_17)\n",
    "df_merged_16 = merge_dataframes(df_stops_16, df_stops_info_16)\n",
    "df_merged_15 = merge_dataframes(df_stops_15, df_stops_info_15)\n",
    "df_merged_14 = merge_dataframes(df_stops_14, df_stops_info_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store merged datasets for easier access\n",
    "years = {\n",
    "    '2017': df_merged_17,\n",
    "    '2016': df_merged_16,\n",
    "    '2015': df_merged_15,\n",
    "    '2014': df_merged_14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a peek at 2017 dataset\n",
    "years['2017'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Fill in datasets with data analitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in all the data at once. Takes roughly 6 mins to run, but will make the rest of the program much faster \n",
    "df_2017 = get_year_areas()\n",
    "df_2016 = get_year_areas()\n",
    "df_2015 = get_year_areas()\n",
    "df_2014 = get_year_areas()\n",
    "\n",
    "get_code_race_data('2017', df_2017)\n",
    "get_code_race_data('2016', df_2016)\n",
    "get_code_race_data('2015', df_2015)\n",
    "get_code_race_data('2014', df_2014)\n",
    "\n",
    "fill_area_pop_data('2017', df_2017)\n",
    "fill_area_pop_data('2016', df_2016)\n",
    "fill_area_pop_data('2015', df_2015)\n",
    "fill_area_pop_data('2014', df_2014)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_years = {\n",
    "    '2017':df_2017,\n",
    "    '2016':df_2016,\n",
    "    '2015':df_2015,\n",
    "    '2014':df_2014\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Population</th>\n",
       "      <td>96512.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>White</th>\n",
       "      <td>61684.0</td>\n",
       "      <td>63.9137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hispanic</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>7.79167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>22111.0</td>\n",
       "      <td>22.9105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black</th>\n",
       "      <td>1171.0</td>\n",
       "      <td>1.21374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two or More</th>\n",
       "      <td>3528.0</td>\n",
       "      <td>3.65581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Indian</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.124648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pacific Islander</th>\n",
       "      <td>117.0</td>\n",
       "      <td>0.121539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>259.0</td>\n",
       "      <td>0.268775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Population   Percent\n",
       "Group                                 \n",
       "Total Population     96512.0       100\n",
       "White                61684.0   63.9137\n",
       "Hispanic              7519.0   7.79167\n",
       "Asian                22111.0   22.9105\n",
       "Black                 1171.0   1.21374\n",
       "Two or More           3528.0   3.65581\n",
       "American Indian        120.0  0.124648\n",
       "Pacific Islander       117.0  0.121539\n",
       "Other                  259.0  0.268775"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_years['2017']['930'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Test dataframe visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Police action counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     W   B   A    H    O\n",
      "Action                                  \n",
      "arrest               3   1   0    2    0\n",
      "310                  1   0   0    0    0\n",
      "impound              7   0   0    2    0\n",
      "tow                  1   0   0    0    0\n",
      "released             0   0   0    0    0\n",
      "suspension notice    1   0   0    0    1\n",
      "check plate          0   0   0    0    0\n",
      "passenger            3   0   0    0    0\n",
      "license              0   0   0    0    0\n",
      "dui eval             1   0   0    0    0\n",
      "detention            0   0   0    1    0\n",
      "contact              0   1   0    0    0\n",
      "suspension           0   0   0    0    0\n",
      "suspect              0   0   0    0    0\n",
      "citation           458  33  96  122  104\n",
      "DMV issue            0   0   0    0    0\n",
      "other                0   0   0    0    0\n",
      "NaN                  0   0   0    0    0\n",
      "total              475  35  96  127  105\n"
     ]
    }
   ],
   "source": [
    "# Police Actions count for police area 930 in 2017\n",
    "print(df_years['2017']['930'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test visual: Race percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Population   Percent\n",
      "Group                                 \n",
      "Total Population     84454.0       100\n",
      "White                65823.0   77.9395\n",
      "Hispanic              8933.0   10.5774\n",
      "Asian                 5813.0   6.88304\n",
      "Black                  870.0   1.03015\n",
      "Two or More           2406.0   2.84889\n",
      "American Indian        162.0   0.19182\n",
      "Pacific Islander       140.0  0.165771\n",
      "Other                  307.0  0.363511\n"
     ]
    }
   ],
   "source": [
    "# Population percentages \n",
    "print(df_years['2017']['120'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Visualization Prep</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Getters for statistics and graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of stops per race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stops_percentage(area, race, year):\n",
    "    df_pop = df_years[year][area][1]\n",
    "    df_actions = df_years[year][area][0]\n",
    "    total = get_total(year, area, df_years[year])\n",
    "    \n",
    "    switch = {\n",
    "        'W': np.float(df_actions.loc['total', race] / total) ,\n",
    "        'B': np.float(df_actions.loc['total', race] / total) ,\n",
    "        'A': np.float(df_actions.loc['total', race] / total) ,\n",
    "        'H': np.float(df_actions.loc['total', race] / total)\n",
    "    }\n",
    "                      \n",
    "    return switch[race] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stops_percentage_graph(area, year):\n",
    "    ratios = list()\n",
    "    for race in races:\n",
    "        ratios.append(get_stops_percentage(area, race, year))\n",
    "\n",
    "    plt.figure(figsize=(6, 4), dpi=75)\n",
    "    plt.title('Percentage of stops corresponding to races\\nYear:'+year+'  Police area:'+area)\n",
    "    plt.xlabel('Race')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.bar(races, ratios, width=.50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population percentages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_population_percentage(area, race, year):\n",
    "    df_pop = df_years[year][area][1]\n",
    "    df_actions = df_years[year][area][0]\n",
    "    total = get_total(year, area, df_years[year])\n",
    "    \n",
    "    switch = {\n",
    "        'W': np.float(df_pop.loc['White', 'Percent']),\n",
    "        'B': np.float(df_pop.loc['Black', 'Percent']),\n",
    "        'A': np.float(df_pop.loc['Asian', 'Percent']),\n",
    "        'H': np.float(df_pop.loc['Hispanic', 'Percent'])\n",
    "    }\n",
    "                      \n",
    "    return switch[race]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_percentage_graph(area, year):\n",
    "    ratios = list()\n",
    "    for race in races:\n",
    "        ratios.append(get_population_percentage(area, race, year))\n",
    "\n",
    "    plt.figure(figsize=(6, 4), dpi=75)\n",
    "    plt.title('Percentage of of race in population\\nYear:'+year+'  Police area:'+area)\n",
    "    plt.xlabel('Race')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.bar(races, ratios, width=.50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Stops per race percentages vs. population percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_compare(area, race, year):\n",
    "    df_pop = df_years[year][area][1]\n",
    "    df_actions = df_years[year][area][0]\n",
    "    total = get_total(year, area, df_years[year])\n",
    "    \n",
    "    switch = {\n",
    "        'W': np.float(df_actions.loc['total', race] / total / np.float(df_pop.loc['White', 'Percent'])),#df_pop.loc['White', 'Population'] / df_actions.loc['total', race],\n",
    "        'B': np.float(df_actions.loc['total', race] / total / np.float(df_pop.loc['Black', 'Percent'])),\n",
    "        'A': np.float(df_actions.loc['total', race] / total) / np.float(df_pop.loc['Asian', 'Percent']),\n",
    "        'H': np.float(df_actions.loc['total', race] / total) / np.float(df_pop.loc['Hispanic', 'Percent'])\n",
    "    }\n",
    "    \n",
    "    return switch[race] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stops_vs_population_graph(area, year):\n",
    "    ratios = list()\n",
    "    for race in races:\n",
    "        ratios.append(race_compare(area, race, year))\n",
    "\n",
    "    plt.figure(figsize=(6, 4), dpi=75)\n",
    "    plt.title('Ratio of stops% vs population%\\nYear:'+year+'  Police area:'+area)\n",
    "    plt.xlabel('Race')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.bar(races, ratios, width=.50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining graphing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulteneous_graph_visual(area, year):\n",
    "    stops_percentage_graph(area,year)\n",
    "    population_percentage_graph(area,year)\n",
    "    stops_vs_population_graph(area,year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Red'>[</font> <font color='Blue'>Data Visualization</font> <font color='Red'>]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='Blue'>-></font> Interactable Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2c366b8dc749608ed1fb031aa23ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_plot = interactive(simulteneous_graph_visual, area=areas_codes_list, year=years_list)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '1000px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
