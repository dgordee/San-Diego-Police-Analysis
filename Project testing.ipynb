{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project links, files, and basic information\n",
    "\n",
    "### Websites with datasets:\n",
    "- San Diego Vehicle Stops:  https://data.sandiego.gov/datasets/police-vehicle-stops/\n",
    "- Dan Diego Population Data:  http://www.city-data.com/city/San-Diego-California.html\n",
    "\n",
    "### Websites of needed information:\n",
    "- San Diego police service areas https://www.sandiego.gov/police/services/divisions (vehcle stop data only records the first two digits)\n",
    "- San Diego zip code map: http://www.city-data.com/zipmaps/San-Diego-California.html\n",
    "\n",
    "### Names of datasets\n",
    "#### *Vehicle Stops*\n",
    "- 'vehicle_stops_2017.csv'\n",
    "- 'vehicle_stops_2016.csv'\n",
    "- 'vehicle_stops_2015.csv'\n",
    "- 'vehicle_stops_2014.csv'\n",
    "\n",
    "#### *Vehicle Stops Details*\n",
    "- 'vehicle_stops_search_details_2017.csv'\n",
    "- 'vehicle_stops_search_details_2016.csv'\n",
    "- 'vehicle_stops_search_details_2015.csv'\n",
    "- 'vehicle_stops_search_details_2014.csv'\n",
    "\n",
    "#### *Files needed to read Vehicle Stops information*\n",
    "- Race Codes: 'vehicle_stops_race_codes.csv'    \n",
    "- Title explanations for Vehicle Stops data: 'vehicle_stops_dictionary.csv'\n",
    "- Title explanations for Vehicle Stops Details data: 'vehicle_stops_search_details_dictionary.csv'\n",
    "- Possible actions taken when stopped for Vehicle Stops Details data: 'vehicle_stops_search_details_description_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gordee\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Web scrapping\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} beautifulsoup4\n",
    "\n",
    "# Data analysis\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "\n",
    "import requests\n",
    "import PyPDF2 as pdf\n",
    "#import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stops = pd.read_csv('vehicle_stops_2017.csv')\n",
    "df_stops_info = pd.read_csv('vehicle_stops_search_details_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31659, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_stops_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The races that make up the majority of the san diego area\n",
    "races = list(['W', 'H', 'A', 'B', 'I', 'O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulls population chart from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zip_pop_data\\\\92025.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c1cdf35ef6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_zip_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'92025'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-c1cdf35ef6d3>\u001b[0m in \u001b[0;36mget_zip_info\u001b[1;34m(code, percent)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrDir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mfpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\PyPDF2\\pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stream, strict, warndest, overwriteWarnings)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PdfFileReader stream/file object is not in binary mode. It may not be read correctly.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfReadWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m             \u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'zip_pop_data\\\\92025.pdf'"
     ]
    }
   ],
   "source": [
    "################# function to get population data from a specific zip code ######################\n",
    "import os\n",
    "import PyPDF2 as pdf\n",
    "import locale\n",
    "\n",
    "def get_zip_info(code, percent):\n",
    "    locale.setlocale(locale.LC_ALL, '')\n",
    "    currDir = 'zip_pop_data\\\\'\n",
    "    try:\n",
    "        file = currDir + code + '.pdf'\n",
    "        fpdf = pdf.PdfFileReader(file)\n",
    "        page = fpdf.getPage(0).extractText()\n",
    "\n",
    "        # Gets the beginning and end of the data we want\n",
    "        index = page.find('Population\\nPercent\\nTotal Population')\n",
    "        indexEnd = page.find('Source: SANDAG, Current Estimates (2010)\\nPopulation by Race')\n",
    "        text = page[index+19:indexEnd-1]\n",
    "        text = list(text)\n",
    "        for index, item in enumerate(text):\n",
    "            if item == \"\\n\":\n",
    "                text[index] = '/'\n",
    "\n",
    "        text = ''.join(text)\n",
    "        text = text.split('/')\n",
    "        groups = list()\n",
    "        percentages = list()\n",
    "        populations = list()\n",
    "        cols = ['Group', 'Population', 'Percent']\n",
    "\n",
    "        for item in text:\n",
    "            if '%' in item:\n",
    "                percentages.append(item)\n",
    "            elif item[0].isnumeric():\n",
    "                populations.append(locale.atoi(item) * np.float(percent))\n",
    "            else:\n",
    "                groups.append(item) \n",
    "\n",
    "        p_df = pd.DataFrame(columns = cols)\n",
    "        p_df['Group'] = groups\n",
    "        p_df['Population'] = populations\n",
    "        p_df['Percent'] = percentages\n",
    "        p_df.set_index('Group', inplace=True)\n",
    "        p_df = p_df.reindex([\"Total Population\", \"White\", \"Hispanic\", \"Asian\", \"Black\", \"Two or More\", \"American Indian\",\n",
    "                    \"Pacific Islander\", \"Other\"])\n",
    "        p_df.fillna(0.0, inplace=True)\n",
    "        \n",
    "        # Makes sure that each value in percent column has a % sign on it - fixes errors caused by null\n",
    "        \n",
    "        for index, row in p_df.iterrows():\n",
    "            if '%' not in str(row['Percent']):\n",
    "                p_df.loc[index, 'Percent'] = str(row['Percent']) + '%'\n",
    "        return p_df\n",
    "    except PermissionError:\n",
    "        print('error')\n",
    "        \n",
    "df = get_zip_info('92025', '.5')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "currDir = 'zip_pop_data\\\\'\n",
    "for currZip in os.listdir('zip_pop_data'):  \n",
    "    try:\n",
    "        file = os.fsdecode(currZip)\n",
    "        fpdf = pdf.PdfFileReader(currDir + file)\n",
    "        page = fpdf.getPage(0).extractText()\n",
    "\n",
    "        # Gets the beginning and end of the data we want\n",
    "        index = page.find('Population\\nPercent\\nTotal Population')\n",
    "        indexEnd = page.find('Source: SANDAG, Current Estimates (2010)\\nPopulation by Race')\n",
    "        text = page[index+19:indexEnd-1]\n",
    "        text = list(text)\n",
    "        for index, item in enumerate(text):\n",
    "            if item == \"\\n\":\n",
    "                text[index] = '/'\n",
    "\n",
    "        text = ''.join(text)\n",
    "        text = text.split('/')\n",
    "        groups = list()\n",
    "        percentages = list()\n",
    "        populations = list()\n",
    "        cols = ['Group', 'Population', 'Percent']\n",
    "\n",
    "        for item in text:\n",
    "            if '%' in item:\n",
    "                percentages.append(item)\n",
    "            elif item[0].isnumeric():\n",
    "                populations.append(item)\n",
    "            else:\n",
    "                groups.append(item) \n",
    "\n",
    "\n",
    "        p_df = pd.DataFrame(columns = cols)\n",
    "        p_df['Group'] = groups\n",
    "        p_df['Population'] = populations\n",
    "        p_df['Percent'] = percentages\n",
    "        print(file)\n",
    "        print(p_df)\n",
    "        print('\\n')\n",
    "    except PermissionError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning stops dataframe - fuctions\n",
    "    ### Clean unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanted column titles for stops dataframe\n",
    "stops_col_titles = ['stop_id','stop_cause','service_area','subject_race','subject_sex','subject_age',\n",
    "                    'arrested','searched','contraband_found','property_seized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_cols(stops):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops),stops_col_titles)\n",
    "    stops.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If nans exist of these columns the entry will be dropped\n",
    "clean_nans_cols = ['stop_cause', 'stop_id', 'subject_race', 'subject_sex', 'subject_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to get rid of nans vehicle stop dataset - Alberto\n",
    "# Params: stops - dataset of stops to clean\n",
    "def clean_stops_nans(stops):\n",
    "    \n",
    "    # Here we assume a Nan means a No in these columns (Since the majority of columns had 'Nan' instead of 'N')\n",
    "    stops['arrested'] = stops['arrested'].replace({np.nan:'N'})\n",
    "    stops['searched'] = stops['searched'].replace({np.nan:'N'})\n",
    "    stops['contraband_found'] = stops['contraband_found'].replace({np.nan:'N'})\n",
    "    stops['property_seized'] = stops['property_seized'].replace({np.nan:'N'})\n",
    "    \n",
    "    stops.dropna(how = 'any', subset = clean_nans_cols, inplace = True)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning stops detail dataframe - functions\n",
    "    Clean unwanted columns of stop details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanted column titles for stops information dataframe\n",
    "stops_info_col_titles = ['stop_id','search_details_type','search_details_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to get rid of unwanted columns in vehicle stop informationdataset - Alberto\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_cols(stops_info):\n",
    "    \n",
    "    #Obtain unwated columns and drop them\n",
    "    drop_list = np.setdiff1d(list(stops_info),stops_info_col_titles)\n",
    "    stops_info.drop(drop_list, axis=1, inplace=True) \n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean NaNs and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take out meaningless entry\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_meaningless(stops_info):\n",
    "    \n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTakenOther') \n",
    "                                      & stops_info['search_details_description'].isnull())]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'ActionTaken') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    stops_info = stops_info[~((stops_info['search_details_type'] == 'SearchBasis') \n",
    "                                      & (stops_info['search_details_description'] == 'Other'))]\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize action type entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_type(action_type):\n",
    "    action_type = str(action_type)\n",
    "    action_type = action_type.lower()\n",
    "    \n",
    "    if 'action' in action_type:\n",
    "        action_type = 'action'\n",
    "    \n",
    "    elif 'search' in action_type:\n",
    "        action_type = 'search'\n",
    "        \n",
    "    return action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize action details entry\n",
    "# Params: action - string to be standarized\n",
    "def standardize_action_desc(action):\n",
    "    \n",
    "    # Otherwise move onto parsinf\n",
    "    action = str(action)\n",
    "    action = action.lower()\n",
    "\n",
    "    if 'nan' in action:\n",
    "        #action = np.nan\n",
    "        action = 'Other'\n",
    "        \n",
    "    elif 'arrest' in action:\n",
    "        action = ['arrest']\n",
    "        \n",
    "    elif '310' in action:\n",
    "        action = ['310']\n",
    "        \n",
    "    elif 'imp' in action:\n",
    "        action = ['impound']\n",
    "\n",
    "    elif 'tow' in action:\n",
    "        action = ['tow']\n",
    "        \n",
    "    elif 'mistake' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'released' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'leave' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'free' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no vio' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'no dui' in action:\n",
    "        action = ['released']\n",
    "        \n",
    "    elif 'nothing' in action:\n",
    "        action = ['released']\n",
    "         \n",
    "    elif 'notice' in action:\n",
    "        action = ['suspension notice']\n",
    "        \n",
    "    elif 'plate' in action:\n",
    "        action = ['check plate']\n",
    "        \n",
    "    elif 'passenger' in action:\n",
    "        action = ['passenger']\n",
    "        \n",
    "    elif 'license' in action:\n",
    "        action = ['license']\n",
    "        \n",
    "    elif 'dui' in action:\n",
    "        action = ['dui eval']\n",
    "        \n",
    "    elif 'det' in action:\n",
    "        action = ['detention']\n",
    "        \n",
    "    elif 'contact' in action:\n",
    "        action = ['contact']\n",
    "        \n",
    "    elif 'suspen' in action:\n",
    "        action = ['suspension']\n",
    "    \n",
    "    elif 'susp' in action:\n",
    "        action = ['suspect']\n",
    "        \n",
    "    elif 'cit' in action:\n",
    "        action = ['citation']\n",
    "        \n",
    "    elif 'dmv' in action:\n",
    "        action = ['DMV issue']\n",
    "        \n",
    "    else:\n",
    "        action = 'Other'\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean nans and reduce descriptions\n",
    "# Params: stops_info - dataset of stops information to clean\n",
    "def clean_stops_info_nans(stops_info):\n",
    "    \n",
    "    # Clean meaningless columns\n",
    "    stops_info = clean_stops_info_meaningless(stops_info)\n",
    "    \n",
    "    # Clean type column\n",
    "    type_title = 'search_details_type'\n",
    "    stops_info[type_title] = stops_info[type_title].apply(standardize_action_type)\n",
    "    \n",
    "    # Clean details column\n",
    "    desc_title = 'search_details_description'\n",
    "    stops_info[desc_title] = stops_info[desc_title].apply(standardize_action_desc)\n",
    "    \n",
    "    # Remove 'Other' and nan entries as they do not give us any extra information\n",
    "    stops_info = stops_info[~(stops_info['search_details_description'] == \"Other\")]\n",
    "    stops_info.dropna(how = 'any', subset = stops_info_col_titles, inplace = True)\n",
    "    \n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops - stops dataframe to be cleaned\n",
    "def clean_stops(stops):\n",
    "    stops = clean_stops_cols(stops)\n",
    "    stops = clean_stops_nans(stops)\n",
    "    \n",
    "    return stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine cleaning dataframe functions into one\n",
    "# Params: stops_info - stops information dataframe to be cleaned\n",
    "def clean_stops_info(stops_info):\n",
    "    stops_info = clean_stops_info_cols(stops_info)\n",
    "    stops_info = clean_stops_info_nans(stops_info)\n",
    "    return stops_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2017 datasets\n",
    "df_stops_17 = pd.read_csv('vehicle_stops_2017.csv')\n",
    "df_stops_17 = clean_stops(df_stops_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2017 datasets\n",
    "df_stops_info_17 = pd.read_csv('vehicle_stops_search_details_2017.csv')\n",
    "df_stops_info_17 = clean_stops_info(df_stops_info_17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the stops and details dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Merges duplicates within the information dataser\n",
    "# Params: info - dataframe with stops information\n",
    "def merge_duplicates(info):\n",
    "    \n",
    "    deleted = 0\n",
    "    last_index = len(info) -1\n",
    "\n",
    "    for index, row in info.iterrows():\n",
    "    \n",
    "        if deleted > 0:\n",
    "            deleted -= 1\n",
    "        \n",
    "        elif index < last_index:\n",
    "        \n",
    "            s_id = row['stop_id']\n",
    "        \n",
    "            next_index = index+1\n",
    "            next_id = info['stop_id'][next_index]\n",
    "    \n",
    "            while (s_id == next_id) & (next_index <= last_index):\n",
    "            \n",
    "                # Grab entry of duplicate\n",
    "                entry = info.loc[next_index, 'search_details_description']\n",
    "            \n",
    "                # Append duplicate entry to original\n",
    "                info.loc[index, 'search_details_description'].append(entry[0])\n",
    "            \n",
    "                # Drop duplicate row\n",
    "                info.drop(next_index, inplace=True)\n",
    "            \n",
    "                # Increase index of next row\n",
    "                next_index += 1\n",
    "            \n",
    "                # Check for out of bounds\n",
    "                if next_index  < last_index:\n",
    "                    next_id = info['stop_id'][next_index]\n",
    "                \n",
    "                deleted += 1\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Merge the stops and details dataframes\n",
    "# Params: stops - dataframe with stops information\n",
    "#          info - dataframe with stop details\n",
    "def merge_dataframes(stops, info):\n",
    "    \n",
    "    # Drop type information\n",
    "    info.drop('search_details_type', axis=1, inplace=True)\n",
    "    \n",
    "    # Reset indeces\n",
    "    info = info.reset_index()\n",
    "    info.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    # Merge duplicates of information dataset\n",
    "    info = merge_duplicates(info)\n",
    "    \n",
    "    df_merged = df_stops_17.merge(df_stops_info_17, on = ['stop_id'], how = 'left')\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in 2017 datasets\n",
    "df_stops_17 = pd.read_csv('vehicle_stops_2014.csv')\n",
    "df_stops_17 = clean_stops(df_stops_17)\n",
    "\n",
    "# Read in 2017 datasets\n",
    "df_stops_info_17 = pd.read_csv('vehicle_stops_search_details_2014.csv')\n",
    "df_stops_info_17 = clean_stops_info(df_stops_info_17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_dataframes(df_stops_17, df_stops_info_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>stop_cause</th>\n",
       "      <th>service_area</th>\n",
       "      <th>subject_race</th>\n",
       "      <th>subject_sex</th>\n",
       "      <th>subject_age</th>\n",
       "      <th>arrested</th>\n",
       "      <th>searched</th>\n",
       "      <th>contraband_found</th>\n",
       "      <th>property_seized</th>\n",
       "      <th>search_details_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1044975</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>110</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1044976</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>320</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>42</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1044977</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>320</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1044978</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>610</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1044980</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>930</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1044979</td>\n",
       "      <td>Equipment Violation</td>\n",
       "      <td>820</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>30</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1044981</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>710</td>\n",
       "      <td>H</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1045139</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>120</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1045141</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>120</td>\n",
       "      <td>W</td>\n",
       "      <td>M</td>\n",
       "      <td>36</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1045140</td>\n",
       "      <td>Moving Violation</td>\n",
       "      <td>120</td>\n",
       "      <td>H</td>\n",
       "      <td>M</td>\n",
       "      <td>27</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>[citation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stop_id           stop_cause service_area subject_race subject_sex  \\\n",
       "0  1044975     Moving Violation          110            W           M   \n",
       "1  1044976     Moving Violation          320            W           M   \n",
       "2  1044977     Moving Violation          320            L           M   \n",
       "3  1044978     Moving Violation          610            W           M   \n",
       "4  1044980  Equipment Violation          930            H           M   \n",
       "5  1044979  Equipment Violation          820            H           M   \n",
       "6  1044981     Moving Violation          710            H           F   \n",
       "7  1045139     Moving Violation          120            W           M   \n",
       "8  1045141     Moving Violation          120            W           M   \n",
       "9  1045140     Moving Violation          120            H           M   \n",
       "\n",
       "  subject_age arrested searched contraband_found property_seized  \\\n",
       "0          24        N        N                N               N   \n",
       "1          42        N        N                N               N   \n",
       "2          29        N        N                N               N   \n",
       "3          23        N        N                N               N   \n",
       "4          35        N        N                N               N   \n",
       "5          30        N        N                N               N   \n",
       "6          19        N        N                N               N   \n",
       "7          32        N        N                N               N   \n",
       "8          36        N        N                N               N   \n",
       "9          27        N        N                N               N   \n",
       "\n",
       "  search_details_description  \n",
       "0                 [citation]  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                 [citation]  \n",
       "4                 [citation]  \n",
       "5                        NaN  \n",
       "6                 [citation]  \n",
       "7                        NaN  \n",
       "8                 [citation]  \n",
       "9                 [citation]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps all of the police race data into appropriate categories that\n",
    "# the census gives us\n",
    "\n",
    "def assign_race(race):\n",
    "# A = asian, B = black, H = hispanic, I = indian, O = other\n",
    "    race_dict = {}\n",
    "\n",
    "    race_dict['A'] = 'A'\n",
    "    race_dict['B'] = 'B'\n",
    "    race_dict['C'] = 'A'\n",
    "    race_dict['D'] = 'A'\n",
    "    race_dict['F'] = 'A'\n",
    "    race_dict['G'] = 'O'\n",
    "    race_dict['H'] = 'H'\n",
    "    race_dict['I'] = 'I'\n",
    "    race_dict['J'] = 'A'\n",
    "    race_dict['K'] = 'A'\n",
    "    race_dict['L'] = 'O' #A?\n",
    "    race_dict['O'] = 'O'\n",
    "   # race_dict['P'] = 'P'\n",
    "    race_dict['P'] = 'O'\n",
    "    race_dict['S'] = 'O' #P?\n",
    "    race_dict['U'] = 'O' #P?\n",
    "    race_dict['V'] = 'A'\n",
    "    race_dict['W'] = 'W'\n",
    "    #race_dict['Z'] = 'T'\n",
    "    race_dict['Z'] = 'O'\n",
    "\n",
    "    race_dict['X'] = 'O'\n",
    "    \n",
    "    return race_dict[race]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['110', '320', '610', '930', '820', '710', '120', '230', '240',\n",
       "       '720', '430', '310', '510', 'Unknown', '810', '440', '830', '520',\n",
       "       '620', '630', '130', '530', '840'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all police areas\n",
    "df_areas = {\n",
    "    '110': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '120': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '130': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '230': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '240': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '310': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '320': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '430': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '440': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '510': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '520': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '530': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '610': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '620': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '630': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '710': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '720': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '810': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '820': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '830': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '840': [pd.DataFrame(), pd.DataFrame()],\n",
    "    '930': [pd.DataFrame(), pd.DataFrame()],\n",
    "    'Unknown': [pd.DataFrame(), pd.DataFrame()]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_areas['110'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts the number of police actions for each race in each police area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      W    B   I    A     H    O\n",
      "Action                                          \n",
      "arrest               50   12   0    3    24    5\n",
      "310                   0    0   0    0     0    2\n",
      "impound              83   12   0    8    91    8\n",
      "tow                   3    0   0    0     2    0\n",
      "released              0    1   0    0     1    0\n",
      "suspension notice     1    0   0    0     0    0\n",
      "check plate           0    0   0    0     0    0\n",
      "passenger            22    9   0    7     6    0\n",
      "license               0    0   0    0     0    0\n",
      "dui eval              1    3   0    1     1    0\n",
      "detention             1    1   0    0     0    0\n",
      "contact               0    0   0    0     0    0\n",
      "suspension            0    0   0    0     1    0\n",
      "suspect               0    0   0    1     2    0\n",
      "citation           2834  213  24  321   875  433\n",
      "DMV issue             0    0   0    0     0    0\n",
      "other                 0    0   0    0     0    0\n",
      "NaN                   0    0   0    0     0    0\n",
      "total              2995  251  24  341  1003  448\n"
     ]
    }
   ],
   "source": [
    "def get_code_race_data(year):\n",
    "    actions = list(['arrest', '310', 'impound', 'tow', 'released', 'suspension notice', 'check plate', 'passenger',\n",
    "                   'license', 'dui eval', 'detention', 'contact', 'suspension', 'suspect', 'citation', 'DMV issue', \n",
    "                   'other', 'NaN', 'total'])\n",
    "\n",
    "    cols = ['W', 'B', 'I', 'A', 'H', 'O']\n",
    "    for df in df_areas:\n",
    "        df_areas[df][0] = pd.DataFrame(columns = cols)\n",
    "        df_areas[df][0]['Action'] = actions\n",
    "        df_areas[df][0].fillna(0, inplace=True)\n",
    "        df_areas[df][0].set_index('Action', inplace=True)\n",
    "\n",
    "    for index, row in df_merged.iterrows():\n",
    "        race = assign_race(row['subject_race'])\n",
    "        desc = row['search_details_description']\n",
    "        area = row['service_area']\n",
    "        if desc is not np.nan:\n",
    "            df_areas[area][0].loc[desc, race] += 1\n",
    "    for item in cols:\n",
    "        for df in df_areas:\n",
    "            df_areas[df][0].loc['total', item] = df_areas[df][0][item].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Population Percent\n",
      "Group                               \n",
      "Total Population    141835.0    100%\n",
      "White                78772.0     55%\n",
      "Hispanic             27326.0     19%\n",
      "Asian                25285.0     17%\n",
      "Black                 4165.0      2%\n",
      "Two or More           4981.0      3%\n",
      "American Indian        389.0     <1%\n",
      "Pacific Islander       489.0     <1%\n",
      "Other                  427.0     <1%\n"
     ]
    }
   ],
   "source": [
    "# Counts the total population for police area\n",
    "# Since each police area covers multiple zip codes, we must loop through all codes in each area\n",
    "def fill_area_pop_data(year):\n",
    "    for area in df_areas:\n",
    "        codes = get_area_zips(area)\n",
    "        if len(codes) is not 1:\n",
    "            df_total = get_zip_info(codes[0], codes[1])\n",
    "            for index in range(2,len(codes)-1, 2):\n",
    "                df_temp = get_zip_info(codes[index], codes[index+1])\n",
    "                df_total = df_total.add(df_temp, fill_value=0)\n",
    "\n",
    "            for index, row in df_total.iterrows():\n",
    "                # Calculates the new percentages of the added zip codes\n",
    "                pop = int((row['Population'] / df_total['Population'][0] * 100))\n",
    "                if pop == 0:\n",
    "                    pop = '<1%'\n",
    "                else:\n",
    "                    pop = str(pop) + '%'\n",
    "                df_total.loc[index, 'Percent'] = pop\n",
    "                # Turning Populations into ints\n",
    "                df_total.loc[index, 'Population'] = int(df_total.loc[index, 'Population'])\n",
    "\n",
    "            df_areas[area][1] = df_total\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92122\n"
     ]
    }
   ],
   "source": [
    "# Function that maps zip codes to police areas\n",
    "def get_area_zips(area):\n",
    "    code_dict = {'110':list(['92122', '1', '92117', '1', '92111', '0.8', '92110', '0.4' ]),\n",
    "                '120': list(['92109', '1', '92037', '1']),\n",
    "                '130': list(['0']),\n",
    "                '230': list(['92129', '1', '92128', '1', '92127', '0.3', '92025', '0.3']),\n",
    "                '240': list(['92145', '1', '92126', '1', '92131', '1']),\n",
    "                '310': list(['92123', '1', '92124', '1', '92108', '1', '92111', '0.2']),\n",
    "                '320': list(['92120', '1', '92119', '1']),\n",
    "                '430': list(['92139', '1', '92114', '1']),\n",
    "                '440': list(['92136', '1', '92102', '0.4', '92113', '0.5']),\n",
    "                '510': list(['92113', '0.5', '92102', '0.6']),\n",
    "                '520': list(['92101', '1']),\n",
    "                '530': list(['0']),\n",
    "                '610': list(['92107', '1', '92106', '1', '92140', '1', '92110', '0.6']),\n",
    "                '620': list(['92103', '1']),\n",
    "                '630': list(['0']),\n",
    "                '710': list(['92173', '1', '92154', '0.4']),\n",
    "                '720': list(['92154', '0.6']),\n",
    "                '810': list(['0']),\n",
    "                '820': list(['92115', '1', '92116', '0.6']),\n",
    "                '830': list(['92105', '0.8']),\n",
    "                '840': list(['0']),\n",
    "                '930': list(['92121', '1', '92130', '1', '92014', '1', '92091', '1', '92127', '0.7']),\n",
    "                'Unknown': list(['0'])\n",
    "                }\n",
    "    return code_dict[area]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_compare(area, race):\n",
    "    df_pop = df_areas[area]\n",
    "    df_actions = \n",
    "    switch = {\n",
    "        'W': df_pop.loc['White', 'Population'] / df_actions.loc['total', race],\n",
    "        'B': df_pop.loc['Black', 'Population'] / df_actions.loc['total', race],\n",
    "        'I': df_pop.loc['American Indian', 'Population'] / df_actions.loc['total', race],\n",
    "        'A': df_pop.loc['Asian', 'Population'] / df_actions.loc['total', race],\n",
    "        'H': df_pop.loc['Hispanic', 'Population'] / df_actions.loc['total', race],\n",
    "        'O': df_pop.loc['Other', 'Population'] / df_actions.loc['total', race],\n",
    "    }\n",
    "    \n",
    "    return switch[race]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 6 artists>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dd9cc5240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratios = list()\n",
    "for race in races:\n",
    "    ratios.append(race_compare('110', race))\n",
    "fig = plt.figure()\n",
    "plt.bar(races, ratios, width=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
